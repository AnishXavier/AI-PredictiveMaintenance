{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Building\n",
    "\n",
    "Using the labeled feature data set constructed in the `Code/2_feature_engineering.ipynb` Jupyter notebook, this notebook loads the data from the Azure Blob container and splits it into a training and test data set. We then build a machine learning model (a decision tree classifier or a random forest classifier) to predict when different components within our machine population will fail. We store the better performing model for deployment in an Azure web service in the next. We will prepare and build the web service in the `Code/4_operationalization.ipynb` Jupyter notebook.\n",
    "\n",
    "**Note:** This notebook will take about 2-4 minutes to execute all cells, depending on the compute configuration you have setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# For some data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# For logging model evaluation parameters back into the\n",
    "# AML Workbench run history plots.\n",
    "import logging\n",
    "#nav from azureml.logging import get_azureml_logger\n",
    "\n",
    "amllog = logging.getLogger(\"azureml\")\n",
    "amllog.level = logging.INFO\n",
    "\n",
    "# Turn on cell level logging.\n",
    "#nav %azureml history on\n",
    "#nav %azureml history show\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run all cells\"\n",
    "tic = time.time()\n",
    "\n",
    "#nav logger = get_azureml_logger() # logger writes to AMLWorkbench runtime view\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Telemetry\n",
    "#nav logger.log('amlrealworld.predictivemaintenance.feature_engineering','true')\n",
    "amllog.info('amlrealworld.predictivemaintenance.feature_engineering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load feature data set\n",
    "\n",
    "We have previously created the labeled feature data set in the `Code\\2_feature_engineering.ipynb` Jupyter notebook. Since the Azure Blob storage account name and account key are not passed between notebooks, you'll need your credentials here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = os.getenv('STAGING_STORAGE_ACCOUNT_NAME')\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY =  os.getenv('STAGING_STORAGE_ACCOUNT_KEY')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the feature engineering note book is stored in the feature engineering container.\n",
    "CONTAINER_NAME = CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# We will store and read each of these data sets in blob storage in an \n",
    "# Azure Storage Container on your Azure subscription.\n",
    "# See https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "# for details.\n",
    "\n",
    "# This is the final feature data file.\n",
    "FEATURES_LOCAL_DIRECT = 'featureengineering_files.parquet'\n",
    "\n",
    "# This is where we store the final model data file.\n",
    "LOCAL_DIRECT = 'model_result.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and dump a short summary of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>dt_truncated</th>\n",
       "      <th>volt_rollingmean_12</th>\n",
       "      <th>rotate_rollingmean_12</th>\n",
       "      <th>pressure_rollingmean_12</th>\n",
       "      <th>vibration_rollingmean_12</th>\n",
       "      <th>volt_rollingmean_24</th>\n",
       "      <th>rotate_rollingmean_24</th>\n",
       "      <th>pressure_rollingmean_24</th>\n",
       "      <th>vibration_rollingmean_24</th>\n",
       "      <th>...</th>\n",
       "      <th>error5sum_rollingmean_24</th>\n",
       "      <th>comp1sum</th>\n",
       "      <th>comp2sum</th>\n",
       "      <th>comp3sum</th>\n",
       "      <th>comp4sum</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "      <th>model_encoded</th>\n",
       "      <th>failure</th>\n",
       "      <th>label_e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 12:00:00</td>\n",
       "      <td>162.374561</td>\n",
       "      <td>445.713044</td>\n",
       "      <td>103.468532</td>\n",
       "      <td>39.696107</td>\n",
       "      <td>166.697820</td>\n",
       "      <td>444.924308</td>\n",
       "      <td>100.427843</td>\n",
       "      <td>40.302193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>169.634236</td>\n",
       "      <td>448.824824</td>\n",
       "      <td>100.134285</td>\n",
       "      <td>40.534216</td>\n",
       "      <td>168.831580</td>\n",
       "      <td>455.688535</td>\n",
       "      <td>98.841978</td>\n",
       "      <td>39.876219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 12:00:00</td>\n",
       "      <td>168.028923</td>\n",
       "      <td>462.552245</td>\n",
       "      <td>97.549672</td>\n",
       "      <td>39.218223</td>\n",
       "      <td>165.477871</td>\n",
       "      <td>454.466625</td>\n",
       "      <td>98.704752</td>\n",
       "      <td>39.480803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-31 00:00:00</td>\n",
       "      <td>162.926820</td>\n",
       "      <td>446.381005</td>\n",
       "      <td>99.859832</td>\n",
       "      <td>39.743383</td>\n",
       "      <td>163.461431</td>\n",
       "      <td>447.678924</td>\n",
       "      <td>101.114095</td>\n",
       "      <td>39.132162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-30 12:00:00</td>\n",
       "      <td>163.996042</td>\n",
       "      <td>448.976843</td>\n",
       "      <td>102.368357</td>\n",
       "      <td>38.520940</td>\n",
       "      <td>167.565612</td>\n",
       "      <td>448.850939</td>\n",
       "      <td>98.799875</td>\n",
       "      <td>38.876751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-30 00:00:00</td>\n",
       "      <td>171.135183</td>\n",
       "      <td>448.725035</td>\n",
       "      <td>95.231392</td>\n",
       "      <td>39.232562</td>\n",
       "      <td>171.374485</td>\n",
       "      <td>444.711628</td>\n",
       "      <td>97.554254</td>\n",
       "      <td>40.444985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-29 12:00:00</td>\n",
       "      <td>171.613787</td>\n",
       "      <td>440.698222</td>\n",
       "      <td>99.877115</td>\n",
       "      <td>41.657408</td>\n",
       "      <td>171.004827</td>\n",
       "      <td>436.349482</td>\n",
       "      <td>101.844735</td>\n",
       "      <td>40.585287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-29 00:00:00</td>\n",
       "      <td>170.395866</td>\n",
       "      <td>432.000742</td>\n",
       "      <td>103.812354</td>\n",
       "      <td>39.513166</td>\n",
       "      <td>172.140074</td>\n",
       "      <td>440.871262</td>\n",
       "      <td>102.942478</td>\n",
       "      <td>39.409219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-28 12:00:00</td>\n",
       "      <td>173.884282</td>\n",
       "      <td>449.741783</td>\n",
       "      <td>102.072602</td>\n",
       "      <td>39.305273</td>\n",
       "      <td>170.610373</td>\n",
       "      <td>448.210517</td>\n",
       "      <td>100.055028</td>\n",
       "      <td>39.664584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>2015-12-28 00:00:00</td>\n",
       "      <td>167.336463</td>\n",
       "      <td>446.679251</td>\n",
       "      <td>98.037455</td>\n",
       "      <td>40.023895</td>\n",
       "      <td>164.276595</td>\n",
       "      <td>448.119379</td>\n",
       "      <td>101.525568</td>\n",
       "      <td>40.340740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>model2</td>\n",
       "      <td>9</td>\n",
       "      <td>(0.0, 0.0, 1.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID        dt_truncated  volt_rollingmean_12  rotate_rollingmean_12  \\\n",
       "0         27 2016-01-01 12:00:00           162.374561             445.713044   \n",
       "1         27 2016-01-01 00:00:00           169.634236             448.824824   \n",
       "2         27 2015-12-31 12:00:00           168.028923             462.552245   \n",
       "3         27 2015-12-31 00:00:00           162.926820             446.381005   \n",
       "4         27 2015-12-30 12:00:00           163.996042             448.976843   \n",
       "5         27 2015-12-30 00:00:00           171.135183             448.725035   \n",
       "6         27 2015-12-29 12:00:00           171.613787             440.698222   \n",
       "7         27 2015-12-29 00:00:00           170.395866             432.000742   \n",
       "8         27 2015-12-28 12:00:00           173.884282             449.741783   \n",
       "9         27 2015-12-28 00:00:00           167.336463             446.679251   \n",
       "\n",
       "   pressure_rollingmean_12  vibration_rollingmean_12  volt_rollingmean_24  \\\n",
       "0               103.468532                 39.696107           166.697820   \n",
       "1               100.134285                 40.534216           168.831580   \n",
       "2                97.549672                 39.218223           165.477871   \n",
       "3                99.859832                 39.743383           163.461431   \n",
       "4               102.368357                 38.520940           167.565612   \n",
       "5                95.231392                 39.232562           171.374485   \n",
       "6                99.877115                 41.657408           171.004827   \n",
       "7               103.812354                 39.513166           172.140074   \n",
       "8               102.072602                 39.305273           170.610373   \n",
       "9                98.037455                 40.023895           164.276595   \n",
       "\n",
       "   rotate_rollingmean_24  pressure_rollingmean_24  vibration_rollingmean_24  \\\n",
       "0             444.924308               100.427843                 40.302193   \n",
       "1             455.688535                98.841978                 39.876219   \n",
       "2             454.466625                98.704752                 39.480803   \n",
       "3             447.678924               101.114095                 39.132162   \n",
       "4             448.850939                98.799875                 38.876751   \n",
       "5             444.711628                97.554254                 40.444985   \n",
       "6             436.349482               101.844735                 40.585287   \n",
       "7             440.871262               102.942478                 39.409219   \n",
       "8             448.210517               100.055028                 39.664584   \n",
       "9             448.119379               101.525568                 40.340740   \n",
       "\n",
       "    ...     error5sum_rollingmean_24  comp1sum  comp2sum  comp3sum  comp4sum  \\\n",
       "0   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "1   ...                          0.0     504.0     564.0     444.0     399.0   \n",
       "2   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "3   ...                          0.0     503.0     563.0     443.0     398.0   \n",
       "4   ...                          0.0     502.0     562.0     442.0     397.0   \n",
       "5   ...                          0.0     502.0     562.0     442.0     397.0   \n",
       "6   ...                          0.0     501.0     561.0     441.0     396.0   \n",
       "7   ...                          0.0     501.0     561.0     441.0     396.0   \n",
       "8   ...                          0.0     500.0     560.0     440.0     395.0   \n",
       "9   ...                          0.0     500.0     560.0     440.0     395.0   \n",
       "\n",
       "    model  age    model_encoded  failure  label_e  \n",
       "0  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "1  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "2  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "3  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "4  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "5  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "6  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "7  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "8  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "9  model2    9  (0.0, 0.0, 1.0)      0.0      0.0  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "# create a local path where we store results\n",
    "if not os.path.exists(FEATURES_LOCAL_DIRECT):\n",
    "    os.makedirs(FEATURES_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if FEATURES_LOCAL_DIRECT in blob.name:\n",
    "        local_file = os.path.join(FEATURES_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "feat_data = spark.read.parquet(FEATURES_LOCAL_DIRECT)\n",
    "feat_data.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Training/Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental practice in machine learning is to calibrate and test your model parameters on data that has not been used to train the model. Evaluation of the model requires splitting the available data into a training portion, a calibration portion and an evaluation portion. Typically, 80% of data is used to train the model and 10% each to calibrate any parameter selection and evaluate your model.\n",
    "\n",
    "In general random splitting can be used, but since time series data have an inherent correlation between observations. For predictive maintenance problems, a time-dependent spliting strategy is often a better approach to estimate performance. For a time-dependent split, a single point in time is chosen, the model is trained on examples up to that point in time, and validated on the examples after that point. This simulates training on current data and score data collected in the future data after the splitting point is not known. However, care must be taken on labels near the split point. In this case, feature records within 7 days of the split point can not be labeled as a failure, since that is unobserved data. \n",
    "\n",
    "In the following code blocks, we split the data at a single point to train and evaluate this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volt_rollingmean_12',\n",
       " 'rotate_rollingmean_12',\n",
       " 'pressure_rollingmean_12',\n",
       " 'vibration_rollingmean_12',\n",
       " 'volt_rollingmean_24',\n",
       " 'rotate_rollingmean_24',\n",
       " 'pressure_rollingmean_24',\n",
       " 'vibration_rollingmean_24',\n",
       " 'volt_rollingmean_36',\n",
       " 'vibration_rollingmean_36',\n",
       " 'rotate_rollingmean_36',\n",
       " 'pressure_rollingmean_36',\n",
       " 'volt_rollingstd_12',\n",
       " 'rotate_rollingstd_12',\n",
       " 'pressure_rollingstd_12',\n",
       " 'vibration_rollingstd_12',\n",
       " 'volt_rollingstd_24',\n",
       " 'rotate_rollingstd_24',\n",
       " 'pressure_rollingstd_24',\n",
       " 'vibration_rollingstd_24',\n",
       " 'volt_rollingstd_36',\n",
       " 'rotate_rollingstd_36',\n",
       " 'pressure_rollingstd_36',\n",
       " 'vibration_rollingstd_36',\n",
       " 'error1sum_rollingmean_24',\n",
       " 'error2sum_rollingmean_24',\n",
       " 'error3sum_rollingmean_24',\n",
       " 'error4sum_rollingmean_24',\n",
       " 'error5sum_rollingmean_24',\n",
       " 'comp1sum',\n",
       " 'comp2sum',\n",
       " 'comp3sum',\n",
       " 'comp4sum',\n",
       " 'age']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define list of input columns for downstream modeling\n",
    "\n",
    "# We'll use the known label, and key variables.\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']\n",
    "\n",
    "# Then get the remaing feature names from the data\n",
    "input_features = feat_data.columns\n",
    "\n",
    "# We'll use the known label, key variables and \n",
    "# a few extra columns we won't need.\n",
    "remove_names = label_var + key_cols + ['failure','model_encoded','model' ]\n",
    "\n",
    "# Remove the extra names if that are in the input_features list\n",
    "input_features = [x for x in input_features if x not in set(remove_names)]\n",
    "\n",
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark models require a vectorized data frame. We transform the dataset here and then split the data into a training and test set. We use this split data to train the model on 9 months of data (training data), and evaluate on the remaining 3 months (test data) going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603000\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "feat_data = va.transform(feat_data).select('machineID','dt_truncated','label_e','features')\n",
    "\n",
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(feat_data)\n",
    "\n",
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(feat_data)\n",
    "\n",
    "# split the data into train/test based on date\n",
    "split_date = \"2015-10-30\"\n",
    "training = feat_data.filter(feat_data.dt_truncated < split_date)\n",
    "testing = feat_data.filter(feat_data.dt_truncated >= split_date)\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models\n",
    "\n",
    "A particualar troubling behavior in predictive maintenance is machine failures are usually rare occurrences compared to normal operation. This is fortunate for the business as maintenance and saftey issues are few, but causes an imbalance in the label distribution. This imbalance leads to poor performance as algorithms tend to classify majority class examples at the expense of minority class, since the total misclassification error is much improved when majority class is labeled correctly. This causes low recall or precision rates, although accuracy can be high. It becomes a larger problem when the cost of false alarms is very high. To help with this problem, sampling techniques such as oversampling of the minority examples can be used. These methods are not covered in this notebook. Because of this, it is also important to look at evaluation metrics other than accuracy alone.\n",
    "\n",
    "We will build and compare two different classification model approaches:\n",
    "\n",
    " - **Decision Tree Classifier**: Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions.\n",
    "\n",
    " - **Random Forest Classifier**: A random forest is an ensemble of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n",
    "\n",
    "We will to compare these models in the AML Workbench _runs_ screen. The next code block creates the model. You can choose between a _DecisionTree_ or _RandomForest_ by setting the 'model_type' variable. We have also included a series of model hyperparameters to guide your exploration of the model space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'RandomForest' # Use 'DecisionTree', or 'RandomForest'\n",
    "\n",
    "# train a model.\n",
    "if model_type == 'DecisionTree':\n",
    "    model = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\",\n",
    "                                      # Maximum depth of the tree. (>= 0) \n",
    "                                      # E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'\n",
    "                                      maxDepth=15,\n",
    "                                      # Max number of bins for discretizing continuous features. \n",
    "                                      # Must be >=2 and >= number of categories for any categorical feature.\n",
    "                                      maxBins=32, \n",
    "                                      # Minimum number of instances each child must have after split. \n",
    "                                      # If a split causes the left or right child to have fewer than \n",
    "                                      # minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.\n",
    "                                      minInstancesPerNode=1, \n",
    "                                      # Minimum information gain for a split to be considered at a tree node.\n",
    "                                      minInfoGain=0.0, \n",
    "                                      # Criterion used for information gain calculation (case-insensitive). \n",
    "                                      # Supported options: entropy, gini')\n",
    "                                      impurity=\"gini\")\n",
    "\n",
    "    ##=======================================================================================================================\n",
    "    #elif model_type == 'GBTClassifier':\n",
    "    #    cls_mthd = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "    ##=======================================================================================================================\n",
    "else:    \n",
    "    model = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", \n",
    "                                      # Passed to DecisionTreeClassifier\n",
    "                                      maxDepth=15, \n",
    "                                      maxBins=32, \n",
    "                                      minInstancesPerNode=1, \n",
    "                                      minInfoGain=0.0,\n",
    "                                      impurity=\"gini\",\n",
    "                                      # Number of trees to train (>= 1)\n",
    "                                      numTrees=50, \n",
    "                                      # The number of features to consider for splits at each tree node. \n",
    "                                      # Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].\n",
    "                                      featureSubsetStrategy=\"sqrt\", \n",
    "                                      # Fraction of the training data used for learning each \n",
    "                                      # decision tree, in range (0, 1].' \n",
    "                                      subsamplingRate = 0.632)\n",
    "\n",
    "# chain indexers and model in a Pipeline\n",
    "pipeline_cls_mthd = Pipeline(stages=[labelIndexer, featureIndexer, model])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_pipeline = pipeline_cls_mthd.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this model, we predict the component failures over the test data set. Since the test set has been created from data the model has not been seen before, it simulates future data. The evaluation then can be generalize to how the model could perform when operationalized and used to score the data in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indexedLabel_prediction</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>119596</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2382</td>\n",
       "      <td>823</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1791</td>\n",
       "      <td>0</td>\n",
       "      <td>627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1055</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  indexedLabel_prediction     0.0  1.0  2.0  3.0  4.0\n",
       "0                     0.0  119596   11   11   27    3\n",
       "1                     1.0    2382  823    1    0    0\n",
       "2                     2.0    1791    0  627    0    0\n",
       "3                     3.0    1055    1    2  424    0\n",
       "4                     4.0     931    0    0    0  315"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions. The Pipeline does all the same operations on the test data\n",
    "predictions = model_pipeline.transform(testing)\n",
    "\n",
    "# Create the confusion matrix for the multiclass prediction results\n",
    "# This result assumes a decision boundary of p = 0.5\n",
    "conf_table = predictions.stat.crosstab('indexedLabel', 'prediction')\n",
    "confuse = conf_table.toPandas()\n",
    "confuse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix lists each true component failure in rows and the predicted value in columns. Labels numbered 0.0 corresponds to no component failures. Labels numbered 1.0 through 4.0 correspond to failures in one of the four components in the machine. As an example, the third number in the top row indicates how many days we predicted component 2 would fail, when no components actually did fail. The second number in the second row, indicates how many days we correctly predicted a component 1 failure within the next 7 days.\n",
    "\n",
    "We read the confusion matrix numbers along the diagonal as correctly classifying the component failures. Numbers above the diagonal indicate the model incorrectly predicting a failure when non occured, and those below indicate incorrectly predicting a non-failure for the row indicated component failure.\n",
    "\n",
    "When evaluating classification models, it is convenient to reduce the results in the confusion matrix into a single performance statistic. However, depending on the problem space, it is impossible to always use the same statistic in this evaluation. Below, we calculate four such statistics.\n",
    "\n",
    "- **Accuracy**: reports how often we correctly predicted the labeled data. Unfortunatly, when there is a class imbalance (a large number of one of the labels relative to others), this measure is biased towards the largest class. In this case non-failure days.\n",
    "\n",
    "Because of the class imbalance inherint in predictive maintenance problems, it is better to look at the remaining statistics instead. Here positive predictions indicate a failure.\n",
    "\n",
    "- **Precision**: Precision is a measure of how well the model classifies the truely positive samples. Precision depends on falsely classifying negative days as positive.\n",
    "\n",
    "- **Recall**: Recall is a measure of how well the model can find the positive samples. Recall depends on falsely classifying positive days as negative.\n",
    "\n",
    "- **F1**: F1 considers both the precision and the recall. F1 score is the harmonic average of precision and recall. An F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "\n",
    "These metrics make the most sense for binary classifiers, though they are still useful for comparision in our multiclass setting. Below we calculate these evaluation statistics for the selected classifier, and post them back to the AML workbench run time page for tracking between experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.951445\n",
      "Precision = 0.975056\n",
      "Recall = 0.262218\n",
      "F1 = 0.413292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "# select (prediction, true label) and compute test error\n",
    "# True positives - diagonal failure terms \n",
    "tp = confuse['1.0'][1]+confuse['2.0'][2]+confuse['3.0'][3]+confuse['4.0'][4]\n",
    "\n",
    "# False positves - All failure terms - True positives\n",
    "fp = np.sum(np.sum(confuse[['1.0', '2.0','3.0','4.0']])) - tp\n",
    "\n",
    "# True negatives \n",
    "tn = confuse['0.0'][0]\n",
    "\n",
    "# False negatives total of non-failure column - TN\n",
    "fn = np.sum(np.sum(confuse[['0.0']])) - tn\n",
    "\n",
    "# Accuracy is diagonal/total \n",
    "acc_n = tn + tp\n",
    "acc_d = np.sum(np.sum(confuse[['0.0','1.0', '2.0','3.0','4.0']]))\n",
    "acc = acc_n/acc_d\n",
    "\n",
    "# Calculate precision and recall.\n",
    "prec = tp/(tp+fp)\n",
    "rec = tp/(tp+fn)\n",
    "\n",
    "# Print the evaluation metrics to the notebook\n",
    "print(\"Accuracy = %g\" % acc)\n",
    "print(\"Precision = %g\" % prec)\n",
    "print(\"Recall = %g\" % rec )\n",
    "print(\"F1 = %g\" % (2.0 * prec * rec/(prec + rec)))\n",
    "print(\"\")\n",
    "\n",
    "# logger writes information back into the AML Workbench run time page.\n",
    "# Each title (i.e. \"Model Accuracy\") can be shown as a graph to track\n",
    "# how the metric changes between runs.\n",
    "amllog.info(\"Model Accuracy\", (acc))\n",
    "amllog.info(\"Model Precision\", (prec))\n",
    "amllog.info(\"Model Recall\", (rec))\n",
    "amllog.info(\"Model F1\", (2.0 * prec * rec/(prec + rec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(34, {0: 0.0417, 1: 0.0743, 2: 0.0182, 3: 0.0217, 4: 0.0333, 5: 0.0346, 6: 0.0208, 7: 0.0236, 8: 0.0324, 9: 0.0145, 10: 0.0364, 11: 0.0115, 12: 0.0084, 13: 0.0095, 14: 0.0089, 15: 0.0095, 16: 0.0115, 17: 0.0197, 18: 0.0137, 19: 0.0144, 20: 0.0109, 21: 0.0137, 22: 0.0113, 23: 0.0119, 24: 0.121, 25: 0.0775, 26: 0.0939, 27: 0.0569, 28: 0.0758, 29: 0.0081, 30: 0.0085, 31: 0.0086, 32: 0.0085, 33: 0.0348})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = model_pipeline.stages[2].featureImportances\n",
    "\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this is a simulated data set. We would expect a model built on real world data to behave very differently. The accuracy may still be close to one, but the precision and recall numbers would be much lower.\n",
    "\n",
    "## Persist the model\n",
    "\n",
    "We'll save the latest model for use in deploying a webservice for operationalization in the next notebook. We store this local to the Jupyter notebook kernel because the model is stored in a hierarchical format that does not translate to Azure Blob storage well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/navig/mnt/azureml-share/pdmrfull.model\n",
      "/home/navig/mnt/azureml-share/pdmrfull.model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.getenv('AZUREML_NATIVE_SHARE_DIRECTORY'), 'pdmrfull.model')\n",
    "model_archive_path = os.path.join(os.getenv('AZUREML_NATIVE_SHARE_DIRECTORY'), 'pdmrfull.model.tar.gz')\n",
    "print(model_path)\n",
    "print(model_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Full run took 2.92 minutes\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "#nav Here we are hardcoding the path since we dont have the env variable set. Comment line and change\n",
    "#nav model_pipeline.write().overwrite().save(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "\n",
    "model_pipeline.write().overwrite().save(model_path)\n",
    "print(\"Model saved\")\n",
    "\n",
    "# Time the notebook execution. \n",
    "# This will only make sense if you \"Run All\" cells\n",
    "toc = time.time()\n",
    "print(\"Full run took %.2f minutes\" % ((toc - tic)/60))\n",
    "amllog.info(\"Model Building Run time\", ((toc - tic)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(model_archive_path, \"w:gz\")\n",
    "tar.add(model_path, arcname=\"pdmrfull.model\")\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the next notebook `Code\\4_operationalization.ipynb` Jupyter notebook we will create the functions needed to operationalize and deploy any model to get realtime predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
